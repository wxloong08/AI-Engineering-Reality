![Embodied AI Banner](./banner.png)
# 学习笔记：具身智能与自动驾驶前沿探索

> **阅读指南**：本笔记分为"入门篇"（第一至四章）和"进阶篇"（第五至六章）。初学者可以先专注入门篇，进阶篇涉及较多技术细节，可在掌握基础后再学习。文末附有术语表供随时查阅。
> 
> ✅ **前置知识**：建议对"神经网络"（Neural Networks）和"大语言模型"（LLM/ChatGPT）的基本原理有最简单的了解。

---

## 一、核心概念与基础对比

### 1.1 什么是具身智能？

想象一下：ChatGPT 很聪明，但它"只是一个脑子"——没有手、没有脚，不能拿东西、不能走路。**具身智能（Embodied AI）** 就是给 AI 装上"身体"，让它能够：

- 👁️ **感知**：通过摄像头、传感器"看到"和"感受"环境
- 🧠 **思考**：分析环境，做出决策
- 🦾 **行动**：控制机械臂、轮子、腿等与物理世界互动

**简单来说**：具身智能 = AI大脑 + 机器人身体 + 与真实世界的互动能力

### 1.2 为什么自动驾驶是"具身智能的明星学生"？

自动驾驶汽车是目前**最成功、规模最大**的具身智能应用——可以把它理解为"自带四个轮子的机器人"。它的成功主要因为：

- **环境相对简单**：道路有车道线、红绿灯、交通规则
- **数据量巨大**：全球数百万辆车每天都在收集驾驶数据
- **商业价值明确**：解决交通安全、解放人类双手

### 1.3 自动驾驶 vs. 人形机器人：技术架构对比

|**维度**|**自动驾驶（例：Tesla FSD）**|**人形机器人（例：Optimus）**|
|---|---|---|
|**核心逻辑**|**做减法（避让）**：核心是不撞到物体|**做加法（交互）**：核心是接触并操作物体|
|**感知器官**|摄像头为主，部分车型有雷达|摄像头、深度相机、**触觉传感器**|
|**自由度**|低（2-3个）：方向盘和油门/刹车|极高（20-50个）：手指、手腕、肘、肩、腰、膝协同|
|**环境特征**|**结构化**：有规则的道路系统|**非结构化**：家庭厨房，随机摆放的物品|
|**核心难点**|高速下的实时决策与极端情况处理|**精细操作**（如拿鸡蛋不碎）与灵活移动|

> 💡 **通俗理解**：自动驾驶像"在棋盘上下棋"——规则明确；人形机器人像"在乱糟糟的房间里找钥匙"——没有固定规则。

---

## 二、核心技术难点与训练方法

### 2.1 莫拉维克悖论（Moravec's Paradox）

这是机器人领域最著名的悖论之一：

> **对人类很难的事（下棋、做微积分）对 AI 很容易；对人类很简单的事（叠衣服、捡钥匙）对 AI 难如登天。**

**为什么会这样？**

人类觉得"简单"的事，其实需要数百万年进化出的感知和运动协调能力。比如叠衣服：

- 需要感知布料的柔软程度
- 需要精确控制每根手指的力度
- 需要实时调整——衣服会变形！

而下棋只需要逻辑推理，不需要和物理世界打交道。

### 2.2 训练困境：虚实迁移鸿沟（Sim-to-Real Gap）

**问题**：真实世界的机器人数据太少、太贵、太危险（机器人摔倒会坏），所以大多数训练在**模拟器（Simulator）** 中进行。

**但模拟器有个大问题**：

- 现实世界的摩擦力、空气阻力、材料形变很难完美模拟
- 真实摄像头有噪点、过曝、模糊，模拟器里的画面太"干净"

结果：在模拟器里训练得很好的机器人，一到真实世界就"水土不服"。

### 2.3 解决方案：域随机化（Domain Randomization）

既然无法完美模拟现实，那就**故意让模拟器变得"更乱"**：

- 随机改变摩擦力（从冰面光滑到砂纸粗糙）
- 随机改变光照（从刺眼阳光到昏暗地下室）
- 随机改变物体颜色和纹理

**目的**：让 AI 见过所有极端情况后，真实世界反而变成"普通难度"。

> 💡 **类比**：就像考试前把历年最难的题都做一遍，正式考试反而觉得简单。

---

## 三、机器人的"大脑"进化：VLA 模型

### 3.1 从"手写规则"到"端到端学习"

**过去的做法**（规则驱动）：

```python
if 看到红灯: 停车()
elif 前方有障碍物: 绕行()
# 现实情况太复杂，规则永远写不完！
```

问题：现实情况太复杂，规则写不完！

**现在的做法**（端到端，End-to-End）：

```mermaid
graph LR
    Input[输入：摄像头像素] --> NeuralNet((神经网络黑盒))
    NeuralNet --> Output[输出：电机控制指令]
```

让 AI 自己从数据中学习"看到什么就该怎么做"。

### 3.2 VLA 模型：让机器人听懂人话

**VLA = Vision（视觉）+ Language（语言）+ Action（动作）**

这是 2024-2025 年机器人领域最热门的技术方向。核心思想是：

> 把 ChatGPT 类的大语言模型**和机器人控制**结合起来，让机器人能听懂自然语言指令。

**工作原理（简化版）**：

1. **视觉编码器**：把摄像头画面转成 AI 能理解的"向量"
2. **语言模型**：理解用户指令（"帮我拿桌上的红牛"）
3. **动作生成器**：输出机器人需要执行的具体动作

**涌现能力示例**：

- 用户说："我累了。"
- AI 推理：用户需要提神 → 红牛可以提神 → 红牛在冰箱里
- 行动：走到冰箱 → 打开门 → 拿出红牛 → 递给用户

### 3.3 2025年主流 VLA 模型一览

|**模型名称**|**开发者**|**特点**|
|---|---|---|
|**Helix**|Figure AI|首个控制人形机器人全身（35自由度）的VLA，采用"快慢双系统"架构|
|**π0 / π0.5**|Physical Intelligence|开源性能标杆，使用Flow Matching生成连续动作|
|**OpenVLA**|斯坦福等高校|开源7B参数模型，支持消费级GPU微调|
|**GR00T N1**|NVIDIA|专为人形机器人设计，支持异构数据训练|
|**Gemini Robotics**|Google DeepMind|基于Gemini 2.0，可完成折纸等高精细任务|

> 💡 **2025年重要趋势**：Figure AI 在2月份宣布结束与 OpenAI 的合作，转向自研 AI 模型 Helix，称"大语言模型正在变得智能但也更加商品化"。

---

## 四、市场格局：巨头对决（2025年12月更新）

### 4.1 传感器之争：纯视觉 vs. 雷达融合

|**流派**|**代表公司**|**技术路线**|**优劣势**|
|---|---|---|---|
|**纯视觉派**|Tesla|只用摄像头，依靠AI"脑补"3D深度|✅ 成本低、易量产 ❌ 极端光照/天气下有挑战|
|**多传感器融合派**|Waymo|摄像头+激光雷达+毫米波雷达|✅ 冗余安全、精度高 ❌ 成本高、依赖高精地图|

**2025年最新进展**：

- Tesla FSD 累计行驶里程已超过 **70亿英里**，正在奥斯汀测试无人驾驶Robotaxi
- Waymo 已在凤凰城、旧金山等城市运营完全无人驾驶出租车，并将于2025年进入东京

### 4.2 人形机器人双雄：Tesla Optimus vs. Figure AI

|**对比维度**|**Tesla Optimus**|**Figure AI**|
|---|---|---|
|**最新进展**|2025年12月展示平滑跑步能力，Gen-3版本预计2026年Q1发布|Figure 03于2025年10月发布，已在BMW工厂部署超1250小时|
|**AI来源**|自研（基于FSD视觉算法）|自研Helix模型（已结束与OpenAI合作）|
|**量产规划**|2025年目标部署5000台，2026年建设百万台产线|BotQ工厂年产能1.2万台，计划2029年出货10万台|
|**目标价格**|$20,000-$30,000|未公开（预计>$100,000）|
|**核心优势**|垂直整合制造+海量驾驶数据复用|率先实现真实工厂商业部署|
|**估值**|（Tesla市值一部分）|$390亿（2025年融资后）|

**Figure AI 在 BMW 的实战成绩**（2025年数据）：

- 在BMW南卡工厂执行钣金装载任务
- 每天完成约1000次放置操作
- 精度达到5毫米以内
- 已参与生产超过30,000辆汽车

> ⚠️ **值得注意**：2025年12月的迈阿密活动上，一台 Optimus 机器人在发放瓶装水时摔倒，摔倒时出现类似"摘VR头盔"的手部动作，引发外界对其是否远程操控的质疑。Tesla尚未正式回应。

---

## 五、【进阶】空间智能技术：3DGS

> ⚠️ **难度提示**：本章涉及计算机图形学专业知识，初学者可先跳过，待熟悉基础概念后再学习。

### 5.1 为什么机器人需要"3D理解"？

前面我们讲了 VLA 模型让机器人能"听懂人话"，但还有一个问题：**机器人如何理解3D空间？**

想象你让机器人"把桌上的杯子放进柜子里"，它需要知道：

- 杯子在空间中的精确位置（x, y, z坐标）
- 柜子门把手的位置
- 移动路径上有没有障碍物

传统方法要么太慢（NeRF），要么太粗糙（点云）。**3D Gaussian Splatting（3DGS）** 是2023年出现的新技术，完美平衡了速度和质量。

### 5.2 3DGS 核心原理（简化版）

**用一句话解释**：用数百万个"带颜色的3D小椭球"来表示整个场景。

**数据结构**：每个高斯球包含：

- 📍 位置（XYZ坐标）
- 🎨 颜色（RGB值）
- 📐 形状（由协方差矩阵定义的椭球形状）
- 👻 透明度（0-1之间）
- ✨ 光泽（通过球谐系数表示不同角度的光照效果）

**渲染过程**：

1. 从某个视角看过去
2. 把所有高斯球"投影"到2D屏幕上
3. 按深度排序，从后往前混合颜色
4. 得到最终图像

### 5.3 3DGS vs. NeRF

|**特性**|**NeRF**|**3DGS**|
|---|---|---|
|表示方式|隐式（神经网络）|显式（高斯点云）|
|渲染速度|慢（~1 FPS）|快（100+ FPS）|
|可编辑性|困难|简单（直接增删改点）|
|训练时间|数小时|数分钟|
|适用场景|离线渲染、电影特效|实时应用、机器人、VR/AR|

### 5.4 2025年 3DGS 在机器人领域的应用

- **SplaTAM / GS-SLAM**：实时建图与定位
- **POGS**：机器人抓取时的物体姿态估计
- **YOPO-Nav**：基于3DGS的视觉导航
- **HAMMER**：多机器人协作建图

> 💡 **2025年里程碑**：Khronos组织（OpenGL标准制定者）宣布将3DGS格式加入glTF生态系统，标志着该技术正式走向工业标准化。

---

## 六、【进阶】语义3D重建：让机器人"看懂"世界

### 6.1 问题：3DGS只知道"形状"，不知道"是什么"

纯粹的3DGS重建出来的场景是"盲"的——它知道某个位置有个红色椭球，但不知道那是"苹果"还是"番茄"。

机器人需要**语义理解**：这是桌子、这是椅子、这是可以抓的杯子。

### 6.2 解决方案：SAM + CLIP + 3DGS 融合

|**模型**|**作用**|**通俗解释**|
|---|---|---|
|**SAM**（Segment Anything Model）|图像分割|在2D照片中"切"出每个物体的轮廓|
|**CLIP**|图文匹配|"认"出轮廓里的物体是什么（桌子？椅子？）|
|**融合训练**|语义注入|把2D的语义标签"教"给3D高斯球|

**最终效果**：每个高斯球不仅有位置和颜色，还有一个"语义ID"，表示它属于哪个物体。

### 6.3 开发者视角：Web 3D Agent 架构示例

如何构建一个在浏览器中运行、能听懂人话的3D场景Agent？

```mermaid
graph TD
    User[👤 用户输入: 把椅子标红] --> BrowserUI(🖥️ 浏览器界面)
    
    subgraph "Web AI Agent 逻辑"
        BrowserUI --> LLM(🤖 LLM解析意图)
        LLM -->|JSON: target='chair', action='highlight'| Logic[JS 语义匹配逻辑]
        
        Logic -->|查找 SemanticID| SplatData[(🧠 3DGS显存数据)]
        SplatData -->|修改颜色| Renderer(🎨 WebGL/WebGPU 渲染器)
    end
    
    Renderer --> Display[🖼️ 最终高亮画面]
    
    style User fill:#f9f,stroke:#333,stroke-width:2px
    style LLM fill:#bbf,stroke:#333,stroke-width:2px
    style Renderer fill:#bfb,stroke:#333,stroke-width:2px
```

---

## 七、【进阶】架构延伸：Web 3D Agent 与具身智能的关系

很多人可能会问：_"Web 3D Agent 看起来只是一个网页 3D 展示，这和造机器人有什么关系？"_

**核心观点**：Web 3D Agent 并不是独立于具身智能的"玩具"，它是具身智能系统的 **"精神时光屋"（训练场）** 和 **"大脑监控室"**。掌握了这个架构，就等于掌握了**通过互联网控制物理世界**的钥匙。

它们之间存在三层紧密关系：

### 7.1 逻辑同构：大脑的低成本试炼场

- **原理**：Web Agent 判断"把红牛标红"的逻辑，与机器人判断"去拿红牛"的大脑逻辑（VLA）是**完全同构**的。两者都需要：
    
    1. 视觉感知（识别场景中的物体）
    2. 语言理解（解析用户意图）
    3. 空间推理（定位目标物体）
    4. 动作规划（执行操作）
- **价值**：在网页里测试语义理解和空间推理，成本远低于操作真实机器人。
    
    - _试错成本_：在网页里 AI 认错椅子只是改错颜色；在现实里 AI 认错椅子可能会把家具撞坏。
    - _迭代速度_：Web 端可以秒级重置场景，真实机器人需要人工复位。
    - _结论_：如果 AI 在网页里连"哪个是椅子"都分不清，就绝不敢让它在现实里去搬椅子。

> 💡 **产业实践**：NVIDIA 专门推出了 "Digital Twins for Physical AI" 学习路径，教开发者如何使用 Isaac Sim 和数字孪生来"simulate, train, and test robots for manufacturing and industrial tasks"。

### 7.2 数字孪生：Sim-to-Real 的桥梁

Web 3D Agent 中使用的 3DGS 技术，是目前做 **数字孪生（Digital Twin）** 的核心。2025年的学术综述明确指出：数字孪生可以"bridge the sim-to-real gap by transforming virtual environments into dynamic and data-rich platforms"。

**典型工作流**：

1. **扫描**：派人或无人机去真实环境（如工厂车间）采集RGB-D数据。
2. **重建**：用 3DGS 生成高保真 3D 场景（.splat 或 .ply 文件）。
3. **云端训练**：把这个 3D 场景加载到仿真平台或 Web Agent，让 AI 在虚拟环境里反复练习任务。
4. **实地部署**：把训练好的"大脑"部署到真实机器人。

**2025年产业落地案例**：

|公司|应用场景|来源|
|---|---|---|
|**Foxconn**|使用 NVIDIA Omniverse 构建 Houston 新工厂的数字孪生，在虚拟环境中优化 AI 基础设施系统的生产流程|NVIDIA GTC 2025|
|**Toyota**|在 Kentucky 工厂使用 idealworks 的 iw.sim（基于 Omniverse）创建数字孪生，探索复杂自动化场景|Engineering.com 2025|
|**Lucid Motors**|构建工厂数字孪生用于实时规划优化，并训练 AI 驱动的机器人系统|Engineering.com 2025|
|**Amazon Robotics**|使用 Omniverse 将机械臂开发周期从"数年缩短到数月"，BlueJay 多臂操作器从概念到生产仅用一年多|Engineering.com 2025|

> 💡 **本质**：你在 Web 浏览器里看到的 3D 场景，本质上就是机器人"脑海"里正在运行的世界模型（World Model）。

### 7.3 遥操作接口：Human-in-the-loop（人在环路）

这是目前自动驾驶和机器人落地最关键的**安全防线**。2025年的研究明确指出："As autonomous driving technologies advance, the need for human-in-the-loop systems becomes increasingly critical to ensure safety, adaptability, and public confidence."

当 AI 遇到边缘情况（Edge Case）时，Web 3D Agent 秒变 **"远程驾驶舱"**：

- **场景**：机器人看到一堆从未见过的奇怪物体，卡住了，不敢动。
- **介入流程**：
    1. 机器人向云端发送当前感知数据。
    2. 人类操作员打开浏览器（Web 3D Agent），看到现场的实时 3D 重建。
    3. **标注**：人类在网页上把障碍物标记为"可忽略"，或画一条路径示意"从这里绕过去"。
    4. **执行**：机器人接收指令，继续工作。
    5. **学习**：这次人类干预的数据被记录，用于后续模型微调。

**为什么用 Web 界面？**

|优势|说明|
|---|---|
|**零部署**|操作员只需浏览器，无需安装专用软件|
|**跨平台**|PC、平板、手机均可接入|
|**低延迟**|WebGPU/WebGL 渲染 + WebSocket 通信，延迟可控制在毫秒级|
|**规模化**|一个操作员可同时监控多台机器人|

> 💡 **前沿技术**：2025年12月发布的 Visionary 平台实现了 WebGPU 驱动的 3DGS 实时渲染，在 RTX 4090 级别硬件上可达 2-16ms/帧，比传统 WebGL 方案快 60-135 倍。

### 7.4 系统全景图解

```mermaid
graph TD
    subgraph Physical["🌍 真实世界 (Physical World)"]
        Reality["真实物理环境"]
        Robot["🦾 机器人/自动驾驶车"]
        Sensor["📷 机载传感器 (RGB-D/LiDAR)"]
        
        Reality <--> Robot
        Robot --> Sensor
    end

    subgraph Cloud["☁️ 云端服务 (Cloud Services)"]
        Reconstruction["🏗️ 3DGS 重建引擎"]
        VLA["🧠 VLA 大模型"]
        Database[("💾 场景数据库")]
        
        Reconstruction --> Database
        Database <--> VLA
    end

    subgraph Web["🌐 Web 端 (Browser)"]
        WebAgent["🖥️ Web 3D Agent"]
        Human["👨‍💻 人类操作员"]
        
        Human --"查看/标注/路径规划"--> WebAgent
    end

    %% 跨层连接
    Sensor --"上传原始数据"--> Reconstruction
    Database --"加载3D场景"--> WebAgent
    WebAgent --"发送用户指令"--> VLA
    VLA --"返回决策/路径"--> WebAgent
    WebAgent --"下发控制指令"--> Robot
    Robot --"上报状态/异常"--> WebAgent

    style WebAgent fill:#f96,stroke:#333,stroke-width:3px
    style VLA fill:#bbf,stroke:#333,stroke-width:2px
```

**图解说明**： 
1. **真实世界层**：机器人在物理环境中运作，通过机载传感器采集 RGB-D 或 LiDAR 数据 
2. **云端服务层**：传感器数据上传后，3DGS 引擎进行 3D 重建，结果存入场景数据库供 VLA 模型调用 
3. **Web 端层**：操作员通过浏览器加载 3D 场景，进行可视化监控和人工干预 
4. **闭环控制**：Web Agent 既能下发控制指令，也能接收机器人的状态上报，形成完整的控制回路

### 7.5 2025年主流技术栈

如果你想动手搭建一个 Web 3D Agent 原型，以下是当前主流的技术选型：

|层级|技术选项|说明|
|---|---|---|
|**仿真平台**|NVIDIA Isaac Sim, Unity, Webots, MuJoCo|用于离线训练和批量数据生成|
|**3D重建**|3DGS (Nerfstudio Splatfacto), COLMAP|从真实场景照片生成 3D 模型|
|**Web渲染**|Visionary (WebGPU), antimatter15/splat (WebGL), Three.js|浏览器端实时渲染|
|**通信中间件**|ROS2 + rosbridge, WebSocket, gRPC-Web|机器人与Web端双向通信|
|**语义理解**|SAM2 + CLIP, OpenVLA, 自定义 VLM|物体分割与识别|

### 7.6 小结：为什么开发者应该关注这个架构？

|你的身份|这个架构对你意味着什么|
|---|---|
|**机器人工程师**|低成本验证感知和规划算法，减少真机调试时间|
|**Web 开发者**|进入具身智能领域的最佳切入点，复用前端技能|
|**AI 研究者**|快速迭代 VLA 模型，可视化调试推理过程|
|**创业者**|远程机器人运维的商业化路径（一个操作员管理 N 台机器人）|

> 📚 **延伸阅读**：
> 
> - [Digital twins to embodied artificial intelligence: review and perspective](https://www.oaepublish.com/articles/ir.2025.11)（2025年3月）
> - [Visionary: WebGPU-Powered Gaussian Splatting Platform](https://arxiv.org/abs/2512.08478)（2025年12月）
> - [NVIDIA Digital Twins for Physical AI Learning Path](https://www.nvidia.com/en-us/learn/learning-path/digital-twins/)

---

### 术语补充（添加到文末术语表）

| 术语     | 英文                       | 解释                              |
| ------ | ------------------------ | ------------------------------- |
| 数字孪生   | Digital Twin             | 物理系统在虚拟空间的高保真副本，支持实时同步、仿真和优化    |
| 人在环路   | Human-in-the-loop (HITL) | 人类保持在控制回路中的系统设计，用于AI决策的监督、干预和纠错 |
| 边缘情况   | Edge Case                | AI系统未见过或难以处理的罕见/极端场景            |
| WebGPU | WebGPU                   | 下一代Web图形API，比WebGL更接近底层GPU，性能更强 |
## 八、商业落地与未来预测

### 8.1 当前落地难点

|**场景**|**难度**|**原因**|
|---|---|---|
|工厂产线|⭐⭐|环境固定、任务重复、容错率较高|
|仓储物流|⭐⭐⭐|物品种类多，但环境相对可控|
|家庭服务|⭐⭐⭐⭐⭐|非结构化环境、柔性物体多、安全要求极高|

### 8.2 演进路线预测

|**阶段**|**时间预估**|**产品形态**|**典型任务**|
|---|---|---|---|
|Phase 1|已实现|单一功能机器人|扫地、割草|
|Phase 2|2025-2027|移动操作员（轮式机械臂）|搬运、简单分拣|
|Phase 3|2028-2030|双足人形机器人|叠衣、做饭、护理|

### 8.3 价格锚点

**$20,000**（约14万人民币）—— 这是 Elon Musk 给出的 Optimus 目标价格，也是进入中产家庭的心理门槛（相当于一辆家用车的价格）。

---

## 术语表

| **术语** | **英文**                                  | **解释**                          |
| ------ | --------------------------------------- | ------------------------------- |
| 具身智能   | Embodied AI                             | 拥有物理身体、能与真实世界交互的AI系统            |
| 端到端    | End-to-End                              | 从原始输入直接到最终输出，中间过程由神经网络自动学习      |
| VLA模型  | Vision-Language-Action                  | 结合视觉、语言、动作的多模态机器人控制模型           |
| Token  | Token                                   | NLP中把文本切成的小块单位，VLA中动作也被表示为Token |
| 域随机化   | Domain Randomization                    | 在模拟器中随机改变环境参数以提高真实世界泛化能力        |
| 3DGS   | 3D Gaussian Splatting                   | 用3D高斯椭球表示场景的高速渲染技术              |
| NeRF   | Neural Radiance Field                   | 用神经网络隐式表示3D场景的技术（3DGS的前辈）       |
| 光栅化    | Rasterization                           | 将3D图形转换为2D像素的渲染过程               |
| 协方差    | Covariance                              | 统计学概念，在3DGS中用于描述椭球的形状和方向        |
| 球谐系数   | Spherical Harmonics                     | 一种数学工具，用于高效表示不同角度的光照颜色变化        |
| SAM    | Segment Anything Model                  | Meta开发的通用图像分割模型                 |
| CLIP   | Contrastive Language-Image Pre-training | OpenAI开发的图文匹配模型                 |
| 高精地图   | HD Map                                  | 包含厘米级精度道路信息的地图（区别于普通导航地图）       |
| VR遥操作  | VR Teleoperation                        | 人类戴VR头盔远程控制机器人，用于收集训练数据         |
| FSD    | Full Self-Driving                       | Tesla的全自动驾驶系统                   |
| LiDAR  | Light Detection and Ranging             | 激光雷达，通过发射激光测量距离生成3D点云           |
| 数字孪生   | Digital Twin                            | 物理系统在虚拟空间的高保真副本，支持实时同步、仿真和优化    |
| 人在环路   | Human-in-the-loop (HITL)                | 人类保持在控制回路中的系统设计，用于AI决策的监督、干预和纠错 |
| 边缘情况   | Edge Case                               | AI系统未见过或难以处理的罕见/极端场景            |
| WebGPU | WebGPU                                  | 下一代Web图形API，比WebGL更接近底层GPU，性能更强 |

---

## 延伸阅读

1. **VLA模型综述**：[Vision-Language-Action Models for Robotics: A Review](https://vla-survey.github.io/)（2025年10月）
2. **3DGS在机器人中的应用**：[3D Gaussian Splatting in Robotics: A Survey](https://arxiv.org/abs/2410.12262)
3. **Figure AI Helix技术博客**：[Helix: A VLA for Generalist Humanoid Control](https://www.figure.ai/news/helix)
4. **OpenVLA开源项目**：[GitHub - openvla/openvla](https://github.com/openvla/openvla)

---

_本笔记最后更新：2025年12月28日_ _内容基于公开资料整理，如有错误欢迎指正_